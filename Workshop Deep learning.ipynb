{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General instructions : \n",
    "- Please answer the questions by inserting your text or your codes into new cells of this notebook.\n",
    "- The test aims at evaluating differents aspects: \n",
    "  - your knowledge in data science\n",
    "  - your ability to provide clean code and visualizations that respects standards\n",
    "  - your ability to search and implement functions on tier libraries\n",
    "  - your ability to find answers in all available resources at your disposal.\n",
    "  \n",
    "This test is adapted from this tutorial : https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "  \n",
    "\n",
    "## Movie review classification\n",
    "\n",
    "In this notebook, you will learn to classify movie reviews as positive or negative, based on the text content of the reviews.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "The IMDB dataset is a set of 50 000 highly polarized reviews from the Internet Movie Database. They’re split into 25 000 reviews for training and 25 000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.\n",
    "\n",
    "The notebook is organized as follows :\n",
    "1. **Loading the IMDB dataset**\n",
    "<br>\n",
    "\n",
    "2. **Preparing the data**\n",
    "<br>\n",
    "\n",
    "3. **Building the network**\n",
    "<br>\n",
    "\n",
    "4. **Evaluating the network**\n",
    "<br>\n",
    "\n",
    "5. **Go further**\n",
    "\n",
    "<br>\n",
    "\n",
    "We load in a single place all the packages and then load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt # data vizualisation\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine reviews are not the same size, passing the argument **num_words=10000** means that we will only keep the top 10 000 most frequently occurring words in the training data.\n",
    "\n",
    "Note that **X_train** and **X_test** are lists of reviews. Each review is a list of word indices (encoding a sequence of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first list of reviews and the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if no word index exceed 10 000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert  is an amazing actor and now the same being director  father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for  and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also  to the two little boy's that played the  of norman and paul they were just brilliant children are often left out of the  list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()  #  dictionary mapping words to an integer index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) # Reverse, mapping integer indices to words\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '') for i in X_train[0]]) # Decodes the review\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have a list of integers and we can't feed that into a neural network. We have to turn our lists into\n",
    "tensors.\n",
    "\n",
    "We can use an **Embedding** layer to handle that or doing **One-hot encoding**.\n",
    "This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s.\n",
    "\n",
    "The function below encode the integer sequences into a binary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "X_train = vectorize_sequences(X_train)\n",
    "X_test = vectorize_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should also vectorize our labels\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions : \n",
    "\n",
    "1. How many layers does this network have ? \n",
    "<br>\n",
    "\n",
    "2. Explain the argument passed to each Dense layer\n",
    "<br>\n",
    "\n",
    "3. What is a sigmoid? Explain why it is well suited for the last layer.\n",
    "<br>\n",
    "\n",
    "4. What is the role of each layer ? \n",
    "<br>\n",
    "\n",
    "5. Write the cross-entropy equation and explain in few sentences why this is indeed a loss (i.e. it decreases when the batch sample predictions are close to the right labels).\n",
    "<br>\n",
    "\n",
    "6. Adam optimizer is a variant of the batch stochastic gradient descent where the learning rate is adjusted according to the dynamic of the learning process. Assuming we would use batch stochastic gradient descent (BSGD) algorithm to optimize the loss, please explain in few sentences the underlying principle of BSGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:10000]\n",
    "partial_x_train = X_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run the training (please be patient)\n",
    "\n",
    "While waiting for training to finish, \n",
    "1. please explain the two parameters (epochs, batch_size) meaning and their potential impacts on the model training. \n",
    "2. explain what is the purpose of the validation_data argument\n",
    "3. is it relevant to use the test set (X_test, y_test) as values for the argument validation_data ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs=10, batch_size=128, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit() returns a __history__ object which is a dictionary containing data about everything that happened during training. \n",
    "<br> Let’s use Matplotlib to plot the training and validation loss side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.93%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the importance of having sufficiently large intermediate layers ?\n",
    "Let’s see what happens when you introduce an information bottleneck by having intermediate layers that are significantly less than 16-dimensional: for example, 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Propose tracks to improve these performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Regularization techniques\n",
    "\n",
    "The goal of this section is to evaluate knowledge on regularization\n",
    "\n",
    "1. Explain what is underfitting and overfitting in few sentences\n",
    "2. Explain the following regularization techniques in few sentences:\n",
    " - L1-regularization\n",
    " - L2-regularization (what is the impact difference vs L1 regularization)\n",
    " - dropout (principle of dropout and impact on parameters)\n",
    "3. Are you aware of other ways used to regularize deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
